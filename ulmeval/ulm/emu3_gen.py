import os
import warnings
from typing import List, Dict, Any
from PIL import Image
import torch
import logging

# ✅ 将 LogitsProcessorList 移到文件顶部（全局作用域）
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoImageProcessor,
    AutoModelForCausalLM,
    GenerationConfig
)
from transformers.generation import (
    LogitsProcessorList,
    UnbatchedClassifierFreeGuidanceLogitsProcessor
)

from .base import BaseModel
from ..smp import splitlen


class Emu3Gen(BaseModel):
    INSTALL_REQ = True
    INTERLEAVE = False

    def __init__(self, model_path="BAAI/Emu3-Gen", vq_path="BAAI/Emu3-VisionTokenizer", **kwargs):
        assert os.path.exists(model_path) or splitlen(model_path) == 2, \
            f"model_path must be a valid local path or HF ID like 'org/model', got {model_path}"

        # --- Emu3 相关的单独 try ---
        try:
            from emu3.mllm.processing_emu3 import Emu3Processor
        except Exception as err:
            import traceback
            traceback.print_exc()
            logging.critical(
                "Failed to import Emu3Processor. Please ensure Emu3 is in PYTHONPATH and dependencies are installed."
            )
            raise err

        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="cuda",
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
            trust_remote_code=True,
        ).eval()

        # Load tokenizer and vision tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side="left")
        image_processor = AutoImageProcessor.from_pretrained(vq_path, trust_remote_code=True)
        image_tokenizer = AutoModel.from_pretrained(
            vq_path, device_map="cuda", trust_remote_code=True
        ).eval()

        self.processor = Emu3Processor(image_processor, image_tokenizer, tokenizer)

        # ✅ Hard-coded generation config (no kwargs merge)
        self.gen_kwargs = {
            "ratio": "1:1",
            "cfg": 3.0,
            "max_new_tokens": 40960,
            "top_k": 2048,
            "do_sample": True,
            "use_cache": True,
            "seed": 42
        }
        warnings.warn(f"Emu3Gen using hard-coded config: {self.gen_kwargs}")
        torch.manual_seed(self.gen_kwargs["seed"])

    def generate_inner(self, message: List[Dict[str, Any]], dataset=None) -> Image.Image:
        prompt = " ".join([msg["value"] for msg in message if msg["type"] == "text"])
        if not prompt:
            raise ValueError("Empty prompt for Emu3Gen (T2I model).")

        POSITIVE_PROMPT = " masterpiece, film grained, best quality."
        NEGATIVE_PROMPT = (
            "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, "
            "fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, "
            "signature, watermark, username, blurry."
        )
        full_prompt = prompt + POSITIVE_PROMPT

        kwargs = dict(
            mode='G',
            ratio=self.gen_kwargs["ratio"],
            image_area=self.model.config.image_area,
            return_tensors="pt",
            padding="longest",
        )
        pos_inputs = self.processor(text=full_prompt, **kwargs)
        neg_inputs = self.processor(text=NEGATIVE_PROMPT, **kwargs)

        device = "cuda"
        pos_ids = pos_inputs.input_ids.to(device)
        pos_mask = pos_inputs.attention_mask.to(device)
        neg_ids = neg_inputs.input_ids.to(device)

        # ✅ 现在 LogitsProcessorList 是全局导入的，可安全使用
        logits_processor = LogitsProcessorList([
            UnbatchedClassifierFreeGuidanceLogitsProcessor(
                self.gen_kwargs["cfg"],
                self.model,
                unconditional_ids=neg_ids,
            )
        ])

        gen_config = GenerationConfig(
            use_cache=self.gen_kwargs["use_cache"],
            eos_token_id=self.model.config.eos_token_id,
            pad_token_id=self.model.config.pad_token_id,
            max_new_tokens=self.gen_kwargs["max_new_tokens"],
            do_sample=self.gen_kwargs["do_sample"],
            top_k=self.gen_kwargs["top_k"],
        )

        with torch.no_grad():
            outputs = self.model.generate(
                pos_ids,
                gen_config,
                logits_processor=logits_processor,
                attention_mask=pos_mask,
            )

        mm_list = self.processor.decode(outputs[0])
        for item in mm_list:
            if isinstance(item, Image.Image):
                return item

        raise RuntimeError("No image generated by Emu3Gen.")

    def batch_generate_inner(self, message: List[Dict[str, Any]], dataset, num_generations: int) -> List[Image.Image]:
        results = []
        for _ in range(num_generations):
            img = self.generate_inner(message, dataset)
            results.append(img)
        return results
